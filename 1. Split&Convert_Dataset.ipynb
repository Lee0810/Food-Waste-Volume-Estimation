{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Split DataSet Food-101 into Train and Test Dataset Based on README Instruction",
   "id": "91d0c3898f72660a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T05:54:20.599019Z",
     "start_time": "2025-11-12T05:49:36.731292Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Copying training images...\n",
      "[train] Copied 500/75750 images...\n",
      "[train] Copied 1000/75750 images...\n",
      "[train] Copied 1500/75750 images...\n",
      "[train] Copied 2000/75750 images...\n",
      "[train] Copied 2500/75750 images...\n",
      "[train] Copied 3000/75750 images...\n",
      "[train] Copied 3500/75750 images...\n",
      "[train] Copied 4000/75750 images...\n",
      "[train] Copied 4500/75750 images...\n",
      "[train] Copied 5000/75750 images...\n",
      "[train] Copied 5500/75750 images...\n",
      "[train] Copied 6000/75750 images...\n",
      "[train] Copied 6500/75750 images...\n",
      "[train] Copied 7000/75750 images...\n",
      "[train] Copied 7500/75750 images...\n",
      "[train] Copied 8000/75750 images...\n",
      "[train] Copied 8500/75750 images...\n",
      "[train] Copied 9000/75750 images...\n",
      "[train] Copied 9500/75750 images...\n",
      "[train] Copied 10000/75750 images...\n",
      "[train] Copied 10500/75750 images...\n",
      "[train] Copied 11000/75750 images...\n",
      "[train] Copied 11500/75750 images...\n",
      "[train] Copied 12000/75750 images...\n",
      "[train] Copied 12500/75750 images...\n",
      "[train] Copied 13000/75750 images...\n",
      "[train] Copied 13500/75750 images...\n",
      "[train] Copied 14000/75750 images...\n",
      "[train] Copied 14500/75750 images...\n",
      "[train] Copied 15000/75750 images...\n",
      "[train] Copied 15500/75750 images...\n",
      "[train] Copied 16000/75750 images...\n",
      "[train] Copied 16500/75750 images...\n",
      "[train] Copied 17000/75750 images...\n",
      "[train] Copied 17500/75750 images...\n",
      "[train] Copied 18000/75750 images...\n",
      "[train] Copied 18500/75750 images...\n",
      "[train] Copied 19000/75750 images...\n",
      "[train] Copied 19500/75750 images...\n",
      "[train] Copied 20000/75750 images...\n",
      "[train] Copied 20500/75750 images...\n",
      "[train] Copied 21000/75750 images...\n",
      "[train] Copied 21500/75750 images...\n",
      "[train] Copied 22000/75750 images...\n",
      "[train] Copied 22500/75750 images...\n",
      "[train] Copied 23000/75750 images...\n",
      "[train] Copied 23500/75750 images...\n",
      "[train] Copied 24000/75750 images...\n",
      "[train] Copied 24500/75750 images...\n",
      "[train] Copied 25000/75750 images...\n",
      "[train] Copied 25500/75750 images...\n",
      "[train] Copied 26000/75750 images...\n",
      "[train] Copied 26500/75750 images...\n",
      "[train] Copied 27000/75750 images...\n",
      "[train] Copied 27500/75750 images...\n",
      "[train] Copied 28000/75750 images...\n",
      "[train] Copied 28500/75750 images...\n",
      "[train] Copied 29000/75750 images...\n",
      "[train] Copied 29500/75750 images...\n",
      "[train] Copied 30000/75750 images...\n",
      "[train] Copied 30500/75750 images...\n",
      "[train] Copied 31000/75750 images...\n",
      "[train] Copied 31500/75750 images...\n",
      "[train] Copied 32000/75750 images...\n",
      "[train] Copied 32500/75750 images...\n",
      "[train] Copied 33000/75750 images...\n",
      "[train] Copied 33500/75750 images...\n",
      "[train] Copied 34000/75750 images...\n",
      "[train] Copied 34500/75750 images...\n",
      "[train] Copied 35000/75750 images...\n",
      "[train] Copied 35500/75750 images...\n",
      "[train] Copied 36000/75750 images...\n",
      "[train] Copied 36500/75750 images...\n",
      "[train] Copied 37000/75750 images...\n",
      "[train] Copied 37500/75750 images...\n",
      "[train] Copied 38000/75750 images...\n",
      "[train] Copied 38500/75750 images...\n",
      "[train] Copied 39000/75750 images...\n",
      "[train] Copied 39500/75750 images...\n",
      "[train] Copied 40000/75750 images...\n",
      "[train] Copied 40500/75750 images...\n",
      "[train] Copied 41000/75750 images...\n",
      "[train] Copied 41500/75750 images...\n",
      "[train] Copied 42000/75750 images...\n",
      "[train] Copied 42500/75750 images...\n",
      "[train] Copied 43000/75750 images...\n",
      "[train] Copied 43500/75750 images...\n",
      "[train] Copied 44000/75750 images...\n",
      "[train] Copied 44500/75750 images...\n",
      "[train] Copied 45000/75750 images...\n",
      "[train] Copied 45500/75750 images...\n",
      "[train] Copied 46000/75750 images...\n",
      "[train] Copied 46500/75750 images...\n",
      "[train] Copied 47000/75750 images...\n",
      "[train] Copied 47500/75750 images...\n",
      "[train] Copied 48000/75750 images...\n",
      "[train] Copied 48500/75750 images...\n",
      "[train] Copied 49000/75750 images...\n",
      "[train] Copied 49500/75750 images...\n",
      "[train] Copied 50000/75750 images...\n",
      "[train] Copied 57500/75750 images...\n",
      "[train] Copied 58000/75750 images...\n",
      "[train] Copied 58500/75750 images...\n",
      "[train] Copied 59000/75750 images...\n",
      "[train] Copied 59500/75750 images...\n",
      "[train] Copied 60000/75750 images...\n",
      "[train] Copied 60500/75750 images...\n",
      "[train] Copied 61000/75750 images...\n",
      "[train] Copied 61500/75750 images...\n",
      "[train] Copied 62000/75750 images...\n",
      "[train] Copied 62500/75750 images...\n",
      "[train] Copied 63000/75750 images...\n",
      "[train] Copied 63500/75750 images...\n",
      "[train] Copied 64000/75750 images...\n",
      "[train] Copied 64500/75750 images...\n",
      "[train] Copied 65000/75750 images...\n",
      "[train] Copied 65500/75750 images...\n",
      "[train] Copied 50500/75750 images...\n",
      "[train] Copied 51000/75750 images...\n",
      "[train] Copied 51500/75750 images...\n",
      "[train] Copied 52000/75750 images...\n",
      "[train] Copied 52500/75750 images...\n",
      "[train] Copied 53000/75750 images...\n",
      "[train] Copied 53500/75750 images...\n",
      "[train] Copied 54000/75750 images...\n",
      "[train] Copied 54500/75750 images...\n",
      "[train] Copied 55000/75750 images...\n",
      "[train] Copied 55500/75750 images...\n",
      "[train] Copied 56000/75750 images...\n",
      "[train] Copied 56500/75750 images...\n",
      "[train] Copied 57000/75750 images...\n",
      "[train] Copied 66000/75750 images...\n",
      "[train] Copied 66500/75750 images...\n",
      "[train] Copied 67000/75750 images...\n",
      "[train] Copied 67500/75750 images...\n",
      "[train] Copied 68000/75750 images...\n",
      "[train] Copied 68500/75750 images...\n",
      "[train] Copied 69000/75750 images...\n",
      "[train] Copied 69500/75750 images...\n",
      "[train] Copied 70000/75750 images...\n",
      "[train] Copied 70500/75750 images...\n",
      "[train] Copied 71000/75750 images...\n",
      "[train] Copied 71500/75750 images...\n",
      "[train] Copied 72000/75750 images...\n",
      "[train] Copied 72500/75750 images...\n",
      "[train] Copied 73000/75750 images...\n",
      "[train] Copied 73500/75750 images...\n",
      "[train] Copied 74000/75750 images...\n",
      "[train] Copied 74500/75750 images...\n",
      "[train] Copied 75000/75750 images...\n",
      "[train] Copied 75500/75750 images...\n",
      "üì¶ Copying validation images (from official test.txt)...\n",
      "[val] Copied 500/25250 images...\n",
      "[val] Copied 1000/25250 images...\n",
      "[val] Copied 1500/25250 images...\n",
      "[val] Copied 2000/25250 images...\n",
      "[val] Copied 2500/25250 images...\n",
      "[val] Copied 3000/25250 images...\n",
      "[val] Copied 3500/25250 images...\n",
      "[val] Copied 4000/25250 images...\n",
      "[val] Copied 4500/25250 images...\n",
      "[val] Copied 5000/25250 images...\n",
      "[val] Copied 5500/25250 images...\n",
      "[val] Copied 6000/25250 images...\n",
      "[val] Copied 6500/25250 images...\n",
      "[val] Copied 7000/25250 images...\n",
      "[val] Copied 7500/25250 images...\n",
      "[val] Copied 8000/25250 images...\n",
      "[val] Copied 8500/25250 images...\n",
      "[val] Copied 9000/25250 images...\n",
      "[val] Copied 9500/25250 images...\n",
      "[val] Copied 10000/25250 images...\n",
      "[val] Copied 10500/25250 images...\n",
      "[val] Copied 11000/25250 images...\n",
      "[val] Copied 11500/25250 images...\n",
      "[val] Copied 12000/25250 images...\n",
      "[val] Copied 12500/25250 images...\n",
      "[val] Copied 13000/25250 images...\n",
      "[val] Copied 13500/25250 images...\n",
      "[val] Copied 14000/25250 images...\n",
      "[val] Copied 14500/25250 images...\n",
      "[val] Copied 15000/25250 images...\n",
      "[val] Copied 15500/25250 images...\n",
      "[val] Copied 16000/25250 images...\n",
      "[val] Copied 17000/25250 images...\n",
      "[val] Copied 17500/25250 images...\n",
      "[val] Copied 18000/25250 images...\n",
      "[val] Copied 16500/25250 images...\n",
      "[val] Copied 18500/25250 images...\n",
      "[val] Copied 19000/25250 images...\n",
      "[val] Copied 19500/25250 images...\n",
      "[val] Copied 20000/25250 images...\n",
      "[val] Copied 20500/25250 images...\n",
      "[val] Copied 21000/25250 images...\n",
      "[val] Copied 21500/25250 images...\n",
      "[val] Copied 22000/25250 images...\n",
      "[val] Copied 22500/25250 images...\n",
      "[val] Copied 23000/25250 images...\n",
      "[val] Copied 23500/25250 images...\n",
      "[val] Copied 24000/25250 images...\n",
      "[val] Copied 24500/25250 images...\n",
      "[val] Copied 25000/25250 images...\n",
      "‚úÖ Done! YOLO dataset created at: C:/Users/User/PycharmProjects/PythonProject/FYP/FoodWasteEstimator/food-101\\food101_yolo\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# === Path settings ===\n",
    "root = r\"C:/Users/User/PycharmProjects/PythonProject/FYP/FoodWasteEstimator/food-101\"   # change this if needed\n",
    "images_dir = os.path.join(root, \"images\")\n",
    "meta_dir = os.path.join(root, \"meta\")\n",
    "\n",
    "output_base = os.path.join(root, \"food101_yolo\")\n",
    "train_dir = os.path.join(output_base, \"train\")\n",
    "val_dir   = os.path.join(output_base, \"val\")\n",
    "\n",
    "# create output folders\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# helper to read split list\n",
    "def read_split(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "# load official splits from meta folder\n",
    "train_list = read_split(os.path.join(meta_dir, \"train.txt\"))\n",
    "test_list  = read_split(os.path.join(meta_dir, \"test.txt\"))  # treat as val\n",
    "\n",
    "def copy_images(split_list, split_name, out_dir):\n",
    "    for i, item in enumerate(split_list):\n",
    "        cls, img_id = item.split('/')\n",
    "        src = os.path.join(images_dir, cls, f\"{img_id}.jpg\")\n",
    "        dst_dir = os.path.join(out_dir, cls)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst_dir)\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"[{split_name}] Copied {i+1}/{len(split_list)} images...\")\n",
    "\n",
    "print(\"Copying training images...\")\n",
    "copy_images(train_list, \"train\", train_dir)\n",
    "\n",
    "print(\"Copying validation images (from official test.txt)...\")\n",
    "copy_images(test_list, \"val\", val_dir)\n",
    "\n",
    "print(\"‚úÖ Done! YOLO dataset created at:\", output_base)\n"
   ],
   "id": "ed506e76442d5c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CONVERT UEC-FOOD-100 INTO YOLO FORMAT & Split Dataset",
   "id": "4a5cec7d0c1e0226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import os, cv2, random, shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# === EDIT THESE ===\n",
    "RAW_ROOT  = Path(r\"C:/Users/User/PycharmProjects/PythonProject/FYP/FoodWasteEstimator/UECFOOD100\")\n",
    "OUT_ROOT  = Path(r\"C:/Users/User/PycharmProjects/PythonProject/FYP/FoodWasteEstimator/UECFOOD100_yolo\")\n",
    "ONE_CLASS = True     # True => single class 'food'; False => keep 100 classes (0..99)\n",
    "SPLIT     = 0.70\n",
    "random.seed(42)\n",
    "\n",
    "for p in [OUT_ROOT/\"images/train\", OUT_ROOT/\"images/val\", OUT_ROOT/\"labels/train\", OUT_ROOT/\"labels/val\"]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def yolo_line(x1,y1,x2,y2,w,h,cls):\n",
    "    xc=((x1+x2)/2)/w; yc=((y1+y2)/2)/h; ww=(x2-x1)/w; hh=(y2-y1)/h\n",
    "    return f\"{cls} {xc:.6f} {yc:.6f} {ww:.6f} {hh:.6f}\\n\"\n",
    "\n",
    "# gather boxes per absolute image path\n",
    "records = defaultdict(list)  # abs_img_path -> list[(cls_id,(x1,y1,x2,y2))]\n",
    "\n",
    "class_dirs = sorted([d for d in RAW_ROOT.iterdir() if d.is_dir() and d.name.isdigit()],\n",
    "                    key=lambda p:int(p.name))\n",
    "\n",
    "for idx, cdir in enumerate(class_dirs, start=1):\n",
    "    cls_id = 0 if ONE_CLASS else (idx-1)\n",
    "    bb = cdir / \"bb_info.txt\"\n",
    "    if not bb.exists():\n",
    "        print(f\"‚ö†Ô∏è  missing {bb}\")\n",
    "        continue\n",
    "\n",
    "    with bb.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        header = f.readline()  # skip \"img x1 y1 x2 y2\"\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            toks = line.split()\n",
    "            if len(toks) < 5:\n",
    "                continue\n",
    "            # parse: img_id x1 y1 x2 y2\n",
    "            img_id = toks[0]\n",
    "            try:\n",
    "                x1, y1, x2, y2 = map(float, toks[1:5])\n",
    "            except:\n",
    "                continue\n",
    "            # map image id -> filename (try several extensions)\n",
    "            candidates = [cdir / f\"{img_id}.jpg\",\n",
    "                          cdir / f\"{img_id}.JPG\",\n",
    "                          cdir / f\"{img_id}.png\",\n",
    "                          cdir / f\"{img_id}.jpeg\"]\n",
    "            img_path = next((p for p in candidates if p.exists()), None)\n",
    "            if img_path is None:\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            # clip\n",
    "            x1 = max(0, min(x1, w-1)); y1 = max(0, min(y1, h-1))\n",
    "            x2 = max(0, min(x2, w-1)); y2 = max(0, min(y2, h-1))\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "\n",
    "            records[str(img_path.resolve())].append((cls_id, (x1,y1,x2,y2)))\n",
    "\n",
    "print(\"Images with boxes:\", len(records))\n",
    "\n",
    "# split by image\n",
    "keys = list(records.keys())\n",
    "random.shuffle(keys)\n",
    "cut = int(SPLIT * len(keys))\n",
    "splits = {\"train\": keys[:cut], \"val\": keys[cut:]}\n",
    "\n",
    "def save_example(abs_path_str, split):\n",
    "    path = Path(abs_path_str)\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is None:\n",
    "        return\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    uniq = f\"{path.stem}_{hash(abs_path_str) & 0xffff:04x}\"\n",
    "    dst_img = OUT_ROOT / f\"images/{split}/{uniq}.jpg\"\n",
    "    dst_lbl = OUT_ROOT / f\"labels/{split}/{uniq}.txt\"\n",
    "\n",
    "    shutil.copyfile(path, dst_img)\n",
    "\n",
    "    lines=[]\n",
    "    for cls_id, (x1,y1,x2,y2) in records[abs_path_str]:\n",
    "        cls = 0 if ONE_CLASS else cls_id\n",
    "        lines.append(yolo_line(x1,y1,x2,y2,w,h,cls))\n",
    "    with dst_lbl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "for split, klist in splits.items():\n",
    "    for k in klist:\n",
    "        save_example(k, split)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\" train imgs:\", len(os.listdir(OUT_ROOT/'images/train')))\n",
    "print(\" val imgs  :\", len(os.listdir(OUT_ROOT/'images/val')))\n"
   ],
   "id": "667a18dc6f49f3dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Quick diagnosisÔºö UEC-FOOD-100 has bounding box?",
   "id": "3597b8f1817f2377"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T17:46:47.001004Z",
     "start_time": "2025-11-12T17:46:46.979241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RAW_ROOT = Path(r\"C:/Users/User/PycharmProjects/PythonProject/FYP/FoodWasteEstimator/UECFOOD100\")\n",
    "\n",
    "# 1) Do we see class folders and bb_info.txt files?\n",
    "cls_dirs = [d for d in RAW_ROOT.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "print(\"Class dirs found:\", len(cls_dirs))\n",
    "missing = []\n",
    "present = []\n",
    "for d in sorted(cls_dirs, key=lambda p: int(p.name)):\n",
    "    f = d / \"bb_info.txt\"\n",
    "    if f.exists(): present.append(f)\n",
    "    else: missing.append(d)\n",
    "\n",
    "print(\"bb_info.txt present:\", len(present))\n",
    "print(\"bb_info.txt missing:\", len(missing))\n",
    "if present:\n",
    "    print(\"\\nSample of first bb_info.txt:\", present[0])\n",
    "    print(\"\\n--- first 10 lines ---\")\n",
    "    try:\n",
    "        for i, line in enumerate(open(present[0], \"r\", encoding=\"utf-8\", errors=\"ignore\")):\n",
    "            print(line.rstrip())\n",
    "            if i>=9: break\n",
    "    except Exception as e:\n",
    "        print(\"Read error:\", e)\n"
   ],
   "id": "c907b4369a214768",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class dirs found: 100\n",
      "bb_info.txt present: 100\n",
      "bb_info.txt missing: 0\n",
      "\n",
      "Sample of first bb_info.txt: C:\\Users\\User\\PycharmProjects\\PythonProject\\FYP\\FoodWasteEstimator\\UECFOOD100\\1\\bb_info.txt\n",
      "\n",
      "--- first 10 lines ---\n",
      "img x1 y1 x2 y2\n",
      "1 0 143 370 486\n",
      "2 20 208 582 559\n",
      "3 2 110 243 410\n",
      "4 0 237 286 536\n",
      "5 8 28 761 585\n",
      "6 0 38 369 310\n",
      "7 0 162 383 450\n",
      "8 80 31 776 454\n",
      "9 2 226 270 470\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
